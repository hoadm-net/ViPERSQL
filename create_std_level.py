#!/usr/bin/env python3
"""
Script t·ªïng h·ª£p ƒë·ªÉ t·∫°o std-level t·ª´ word-level.
Bao g·ªìm:
1. T·∫°o tables.json (chu·∫©n h√≥a schema)
2. T·∫°o train.json, dev.json, test.json (chu·∫©n h√≥a d·ªØ li·ªáu)
"""

import json
import re
from pathlib import Path

def load_json_file(file_path):
    """Load JSON file."""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json_file(data, file_path):
    """Save data to JSON file."""
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
    file_path.parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def normalize_token(token):
    """
    Normalize a token from word-level to std-level format.
    - Replace underscores with spaces first, then convert to snake_case
    """
    # First, replace underscores with spaces to get the original form
    token_with_spaces = token.replace('_', ' ')
    
    # Then convert to proper snake_case
    # Split by spaces and join with underscores
    parts = token_with_spaces.split()
    normalized = '_'.join(parts)
    
    return normalized

def fix_quoted_strings(query):
    """
    Fix quoted strings in query: replace underscores with spaces inside quotes.
    """
    def replace_in_quotes(match):
        quoted_content = match.group(1)
        # Replace underscores with spaces inside quotes
        fixed_content = quoted_content.replace('_', ' ')
        return f'"{fixed_content}"'
    
    # Find all quoted strings and fix them
    pattern = r'"([^"]*)"'
    fixed_query = re.sub(pattern, replace_in_quotes, query)
    
    return fixed_query

def normalize_question(question):
    """
    Normalize question: replace all underscores with spaces.
    """
    return question.replace('_', ' ')

def normalize_schema(word_schema):
    """
    Normalize schema from word-level to std-level format.
    """
    normalized_schema = {}
    
    # Copy basic fields
    for key in ['db_id', 'foreign_keys', 'primary_keys']:
        if key in word_schema:
            normalized_schema[key] = word_schema[key]
    
    # Normalize table names
    normalized_schema['table_names'] = []
    normalized_schema['table_names_original'] = []
    for table_name in word_schema['table_names']:
        normalized_name = normalize_token(table_name)
        normalized_schema['table_names'].append(normalized_name)
        normalized_schema['table_names_original'].append(normalized_name)
    
    # Normalize column names
    normalized_schema['column_names'] = []
    normalized_schema['column_names_original'] = []
    for col_info in word_schema['column_names']:
        table_idx, col_name = col_info
        normalized_col_name = normalize_token(col_name)
        normalized_schema['column_names'].append([table_idx, normalized_col_name])
        normalized_schema['column_names_original'].append([table_idx, normalized_col_name])
    
    # Normalize column types
    if 'column_types' in word_schema:
        normalized_schema['column_types'] = word_schema['column_types']
    
    return normalized_schema

def process_query_toks(query_toks):
    """
    Process query_toks and normalize tokens using simple replace.
    """
    normalized_toks = []
    
    for token in query_toks:
        # Simply normalize each token
        normalized_toks.append(normalize_token(token))
    
    return normalized_toks

def normalize_data(word_level_data):
    """
    Normalize data from word-level to std-level format.
    """
    normalized_data = []
    
    for word_item in word_level_data:
        # Process query_toks
        normalized_toks = process_query_toks(word_item['query_toks'])
        
        # Join tokens to create normalized query
        normalized_query = ' '.join(normalized_toks)
        
        # Fix quoted strings (replace underscores with spaces inside quotes)
        normalized_query = fix_quoted_strings(normalized_query)
        
        # Normalize question (replace underscores with spaces)
        normalized_question = normalize_question(word_item['question'])
        
        # Create normalized item
        normalized_item = {
            'db_id': word_item['db_id'],
            'question': normalized_question,
            'query': normalized_query
        }
        
        normalized_data.append(normalized_item)
    
    return normalized_data

def create_std_level():
    """
    Main function to create std-level from word-level.
    """
    print("üöÄ B·∫Øt ƒë·∫ßu t·∫°o std-level t·ª´ word-level...")
    
    # File paths
    word_level_dir = Path('dataset/ViText2SQL/word-level')
    std_level_dir = Path('dataset/ViText2SQL/std-level')
    
    # Input files
    word_tables = word_level_dir / 'tables.json'
    word_train = word_level_dir / 'train.json'
    word_dev = word_level_dir / 'dev.json'
    word_test = word_level_dir / 'test.json'
    
    # Output files
    std_tables = std_level_dir / 'tables.json'
    std_train = std_level_dir / 'train.json'
    std_dev = std_level_dir / 'dev.json'
    std_test = std_level_dir / 'test.json'
    
    # 1. T·∫°o tables.json
    print("\nüìã B∆∞·ªõc 1: T·∫°o tables.json...")
    word_schemas = load_json_file(word_tables)
    std_schemas = []
    
    for schema in word_schemas:
        normalized_schema = normalize_schema(schema)
        std_schemas.append(normalized_schema)
    
    save_json_file(std_schemas, std_tables)
    print(f"‚úÖ ƒê√£ t·∫°o {std_tables}")
    
    # 2. T·∫°o train.json
    print("\nüìö B∆∞·ªõc 2: T·∫°o train.json...")
    if word_train.exists():
        word_train_data = load_json_file(word_train)
        normalized_train = normalize_data(word_train_data)
        save_json_file(normalized_train, std_train)
        print(f"‚úÖ ƒê√£ t·∫°o {std_train} v·ªõi {len(normalized_train)} samples")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y train.json trong word-level")
    
    # 3. T·∫°o dev.json
    print("\nüß™ B∆∞·ªõc 3: T·∫°o dev.json...")
    if word_dev.exists():
        word_dev_data = load_json_file(word_dev)
        normalized_dev = normalize_data(word_dev_data)
        save_json_file(normalized_dev, std_dev)
        print(f"‚úÖ ƒê√£ t·∫°o {std_dev} v·ªõi {len(normalized_dev)} samples")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y dev.json trong word-level")
    
    # 4. T·∫°o test.json
    print("\nüß™ B∆∞·ªõc 4: T·∫°o test.json...")
    if word_test.exists():
        word_test_data = load_json_file(word_test)
        normalized_test = normalize_data(word_test_data)
        save_json_file(normalized_test, std_test)
        print(f"‚úÖ ƒê√£ t·∫°o {std_test} v·ªõi {len(normalized_test)} samples")
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y test.json trong word-level")
    
    # 5. Ki·ªÉm tra k·∫øt qu·∫£
    print("\nüîç B∆∞·ªõc 5: Ki·ªÉm tra k·∫øt qu·∫£...")
    
    # Ki·ªÉm tra m·ªôt s·ªë v√≠ d·ª•
    if word_dev.exists() and std_dev.exists():
        word_dev_data = load_json_file(word_dev)
        std_dev_data = load_json_file(std_dev)
        
        print("\nüìä V√≠ d·ª• chu·∫©n h√≥a:")
        for i, (word_item, std_item) in enumerate(zip(word_dev_data[:3], std_dev_data[:3])):
            print(f"\nV√≠ d·ª• {i+1}:")
            print(f"  Word-level question: {word_item['question']}")
            print(f"  Std-level question: {std_item['question']}")
            print(f"  Word-level query_toks: {word_item['query_toks'][:10]}...")
            print(f"  Std-level query: {std_item['query'][:100]}...")
    
    print(f"\nüéâ Ho√†n th√†nh! Std-level ƒë√£ ƒë∆∞·ª£c t·∫°o t·∫°i: {std_level_dir}")
    print(f"üìÅ C√°c file ƒë√£ t·∫°o:")
    print(f"   - {std_tables}")
    print(f"   - {std_train}")
    print(f"   - {std_dev}")
    print(f"   - {std_test}")

if __name__ == "__main__":
    create_std_level() 